{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91932\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91932\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkmodel = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
    "\n",
    "tokenizer45 = AutoTokenizer.from_pretrained(checkmodel)\n",
    "model45 = AutoModel.from_pretrained(checkmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tokens(sentences, tokenizer):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    \n",
    "    # encoding all sentences for bert input\n",
    "    for sentence in sentences:\n",
    "        sentence_encoding = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            max_length=128,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids.append(sentence_encoding['input_ids'][0])\n",
    "        attention_mask.append(sentence_encoding['attention_mask'][0])\n",
    "    \n",
    "    # stacking all the input_ids and attention_mask along 1 dim\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    attention_mask = torch.stack(attention_mask)\n",
    "    # final shape of input_ids & attention_mask = torch.Size([6, 128]), initially they were list.\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_vector(tokens, model):\n",
    "    \n",
    "    last_hidden_state, pooled_output = model(**tokens, return_dict=False)\n",
    "    '''\n",
    "    Now let's apply mean pooling on last_hidden_state vector of shape torch.Size([6,128,768])\n",
    "    to convert it into meaningful sentence embedding.\n",
    "\n",
    "    For this we need to create a sentence vector by multiplying the attention_mask\n",
    "    with last_hidden_state so that we ignore non-real tokens i.e. ignore padding tokens. \n",
    "    \n",
    "    The final sentence vector will have 768 embeddings for those\n",
    "    words where there was 1 else 0 = padding tokens.\n",
    "    In order to multiply, we need to expand attention_mask dim by 1 so that both becomes [6,128,768].\n",
    "    '''    \n",
    "    attention_mask = tokens['attention_mask'].unsqueeze(-1).expand(last_hidden_state.shape).float()\n",
    "    masked_embeddings = last_hidden_state * attention_mask \n",
    "\n",
    "    # applying mean pooling\n",
    "    '''\n",
    "    This pooling operation will take the mean of all token embeddings and compress them into a \n",
    "    single 768 vector space — creating a ‘sentence vector’.\n",
    "\n",
    "    At the same time, we can’t just take the mean activation as is. We need to consider \n",
    "    null padding tokens (which we should not include).\n",
    "    '''\n",
    "    summed = torch.sum(masked_embeddings, dim=1) # shape = [6,768]\n",
    "    counts = torch.clamp(attention_mask.sum(dim=1), min=1e-9) # shape = [6,768]\n",
    "    mean_pooled_embedding = summed / counts # shape = [6, 768] i.e. our final sentence vector.\n",
    "\n",
    "    return mean_pooled_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(sentences, tokenizer, model):\n",
    "    \n",
    "    sentences_tokens = compute_tokens(sentences, tokenizer)\n",
    "    sentences_embeddings = compute_sentence_vector(sentences_tokens, model)\n",
    "    sentences_embeddings_detached = sentences_embeddings.detach().numpy()\n",
    "    similarity_scores = cosine_similarity([sentences_embeddings_detached[0]], sentences_embeddings_detached[1:])\n",
    "\n",
    "    d = {\n",
    "        'column-1': [sentences[0] for _ in range(len(sentences)-1)],\n",
    "        'column-2': [sent for sent in sentences[1:]],\n",
    "        'scores': similarity_scores[0]\n",
    "    }\n",
    "\n",
    "    output = pd.DataFrame(data=d)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "             \"Three years later, the coffin was still full of Jello.\",\n",
    "             \"The fish dreamed of escaping the fishbowl and into the toilet where he saw his friend go.\",\n",
    "             \"The person box was packed with jelly many dozens of months later.\",\n",
    "             \"Standing on one's head at job interviews forms a lasting impression.\",\n",
    "             \"It took him a month to finish the meal.\",\n",
    "             \"Finishing the meal took him 3 weeks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =[\"We're sorry to hear about your experience\",\"We're sorry to hear that you had a negative experience with our food delivery service.\",\"We take all feedback seriously and are committed to providing the best possible service to our customers.\",\"We would like to apologize for any inconvenience or frustration that you may have experienced.\",\"We understand that mistakes can happen, but we are dedicated to making things right for our customers.\",\"Please let us know more about the issue you faced so that we can investigate and take appropriate action.\",\"We value your feedback and want to ensure that we are meeting your expectations.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = compute_similarity(sentences, tokenizer45, model45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"We're sorry to hear about your experience\",\n",
       " \"We're sorry to hear that you had a negative experience with our food delivery service.\",\n",
       " 'We take all feedback seriously and are committed to providing the best possible service to our customers.',\n",
       " 'We would like to apologize for any inconvenience or frustration that you may have experienced.',\n",
       " 'We understand that mistakes can happen, but we are dedicated to making things right for our customers.',\n",
       " 'Please let us know more about the issue you faced so that we can investigate and take appropriate action.',\n",
       " 'We value your feedback and want to ensure that we are meeting your expectations.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column-1</th>\n",
       "      <th>column-2</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>We're sorry to hear that you had a negative ex...</td>\n",
       "      <td>0.718817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>We take all feedback seriously and are committ...</td>\n",
       "      <td>0.253664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>We would like to apologize for any inconvenien...</td>\n",
       "      <td>0.769503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>We understand that mistakes can happen, but we...</td>\n",
       "      <td>0.534049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>Please let us know more about the issue you fa...</td>\n",
       "      <td>0.494491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We're sorry to hear about your experience</td>\n",
       "      <td>We value your feedback and want to ensure that...</td>\n",
       "      <td>0.435926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    column-1  \\\n",
       "0  We're sorry to hear about your experience   \n",
       "1  We're sorry to hear about your experience   \n",
       "2  We're sorry to hear about your experience   \n",
       "3  We're sorry to hear about your experience   \n",
       "4  We're sorry to hear about your experience   \n",
       "5  We're sorry to hear about your experience   \n",
       "\n",
       "                                            column-2    scores  \n",
       "0  We're sorry to hear that you had a negative ex...  0.718817  \n",
       "1  We take all feedback seriously and are committ...  0.253664  \n",
       "2  We would like to apologize for any inconvenien...  0.769503  \n",
       "3  We understand that mistakes can happen, but we...  0.534049  \n",
       "4  Please let us know more about the issue you fa...  0.494491  \n",
       "5  We value your feedback and want to ensure that...  0.435926  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
