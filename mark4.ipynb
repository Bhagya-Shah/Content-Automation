{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91932\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     attention_masks\u001b[39m.\u001b[39mappend([\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m max_length)\n\u001b[0;32m     42\u001b[0m     labels\u001b[39m.\u001b[39mappend(label_tokenized)\n\u001b[1;32m---> 44\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(input_ids)\n\u001b[0;32m     45\u001b[0m attention_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(attention_masks)\n\u001b[0;32m     46\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# Load the sentiment-response dataset\n",
    "df = pd.read_excel('result.xlsx')\n",
    "sentiments = df['sentiment'].tolist()\n",
    "responses = df['response'].tolist()\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on the sentiment-response dataset\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "epochs = 3\n",
    "max_length = 512\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Encode the sentiment and response\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response_tokenized = tokenizer.encode(responses[i], add_special_tokens=False)\n",
    "    input_tokenized = sentiment_tokenized + response_tokenized\n",
    "    label_tokenized = response_tokenized\n",
    "\n",
    "    # Truncate or pad the token sequences\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    label_tokenized = label_tokenized[:max_length]\n",
    "    padding_length = max_length - len(input_tokenized)\n",
    "    input_tokenized = input_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "    label_tokenized = label_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids.append(input_tokenized)\n",
    "    attention_masks.append([1] * max_length)\n",
    "    labels.append(label_tokenized)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_masks, labels = tuple(t.to(model.device) for t in batch)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = loss_fn(outputs[1], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generate email responses and compute BLEU score\n",
    "generated_responses = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Generate a response from the model\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response = responses[i]\n",
    "    input_tokenized = sentiment_tokenized + tokenizer.encode(response, add_special_tokens=False)\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    input_ids = torch.tensor([input_tokenized]).to(model.device)\n",
    "    generated_output = model.generate(input_ids=input_ids, max_length=max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "    generated_response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    generated_responses.append(generated_response)\n",
    "\n",
    "# Compute the BLEU score between the generated and true responses\n",
    "bleu_scores = []\n",
    "for generated_response, true_response in zip(generated_responses, responses):\n",
    "    bleu_score = sentence_bleu([true_response], generated_response)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# print(f\"Average BLEU score: {\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# Load the sentiment-response dataset\n",
    "df = pd.read_excel('result.xlsx')\n",
    "sentiments = df['sentiment'].tolist()\n",
    "responses = df['response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m     attention_masks\u001b[39m.\u001b[39mappend([\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m max_length)\n\u001b[0;32m     32\u001b[0m     labels\u001b[39m.\u001b[39mappend(label_tokenized)\n\u001b[1;32m---> 34\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(input_ids)\n\u001b[0;32m     35\u001b[0m attention_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(attention_masks)\n\u001b[0;32m     36\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on the sentiment-response dataset\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "epochs = 3\n",
    "max_length = 512\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Encode the sentiment and response\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response_tokenized = tokenizer.encode(responses[i], add_special_tokens=False)\n",
    "    input_tokenized = sentiment_tokenized + response_tokenized\n",
    "    label_tokenized = response_tokenized\n",
    "\n",
    "    # Truncate or pad the token sequences\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    label_tokenized = label_tokenized[:max_length]\n",
    "    padding_length = max_length - len(input_tokenized)\n",
    "    input_tokenized = input_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "    label_tokenized = label_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids.append(input_tokenized)\n",
    "    attention_masks.append([1] * max_length)\n",
    "    labels.append(label_tokenized)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_masks, labels = tuple(t.to(model.device) for t in batch)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = loss_fn(outputs[1], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generate email responses and compute BLEU score\n",
    "generated_responses = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Generate a response from the model\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response = responses[i]\n",
    "    input_tokenized = sentiment_tokenized + tokenizer.encode(response, add_special_tokens=False)\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    input_ids = torch.tensor([input_tokenized]).to(model.device)\n",
    "    generated_output = model.generate(input_ids=input_ids, max_length=max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "    generated_response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    generated_responses.append(generated_response)\n",
    "\n",
    "# Compute the BLEU score between the generated and true responses\n",
    "bleu_scores = []\n",
    "for generated_response, true_response in zip(generated_responses, responses):\n",
    "    bleu_score = sentence_bleu([true_response], generated_response)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     attention_masks\u001b[39m.\u001b[39mappend([\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m max_length)\n\u001b[0;32m     42\u001b[0m     labels\u001b[39m.\u001b[39mappend(label_tokenized)\n\u001b[1;32m---> 44\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(input_ids)\n\u001b[0;32m     45\u001b[0m attention_masks \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(attention_masks)\n\u001b[0;32m     46\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "# from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# # Load the sentiment-response dataset\n",
    "# df = pd.read_csv('sentiment_responses.csv')\n",
    "# sentiments = df['sentiment'].tolist()\n",
    "# responses = df['response'].tolist()\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model on the sentiment-response dataset\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "epochs = 3\n",
    "max_length = 512\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Encode the sentiment and response\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response_tokenized = tokenizer.encode(responses[i], add_special_tokens=False)\n",
    "    input_tokenized = sentiment_tokenized + response_tokenized\n",
    "    label_tokenized = response_tokenized\n",
    "\n",
    "    # Truncate or pad the token sequences\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    label_tokenized = label_tokenized[:max_length]\n",
    "    padding_length = max_length - len(input_tokenized)\n",
    "    input_tokenized = input_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "    label_tokenized = label_tokenized + [tokenizer.pad_token_id] * padding_length\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    input_ids.append(input_tokenized)\n",
    "    attention_masks.append([1] * max_length)\n",
    "    labels.append(label_tokenized)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "attention_masks = torch.tensor(attention_masks)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_masks, labels = tuple(t.to(model.device) for t in batch)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = loss_fn(outputs[1], labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generate email responses and compute BLEU score\n",
    "generated_responses = []\n",
    "for i in range(len(sentiments)):\n",
    "    # Generate a response from the model\n",
    "    sentiment_tokenized = tokenizer.encode(sentiments[i], add_special_tokens=False)\n",
    "    response = responses[i]\n",
    "    input_tokenized = sentiment_tokenized + tokenizer.encode(response, add_special_tokens=False)\n",
    "    input_tokenized = input_tokenized[:max_length]\n",
    "    input_ids = torch.tensor([input_tokenized]).to(model.device)\n",
    "    generated_output = model.generate(input_ids=input_ids, max_length=max_length, pad_token_id=tokenizer.pad_token_id)\n",
    "    generated_response = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    generated_responses.append(generated_response)\n",
    "\n",
    "# Compute the BLEU score between the generated and true responses\n",
    "bleu_scores = []\n",
    "for generated_response, true_response in zip(generated_responses, responses):\n",
    "    bleu_score = sentence_bleu([true_response], generated_response)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "# print(f\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mtensor(input_ids)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "torch.tensor(input_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
